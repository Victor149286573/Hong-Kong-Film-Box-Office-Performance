{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPHtu7xKrmQGenPbO/erAX2",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Victor149286573/Hong-Kong-Film-Box-Office-Performance/blob/master/bridgewater.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this part, we focus on\n",
        "# Bridgewater Associates, LP: CIK Number - 0001350694\n"
      ],
      "metadata": {
        "id": "OR7lMR2xQOif"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Should Connect to the local notebook**\n",
        "1.   jupyter notebook --NotebookApp.allow_origin='https://colab.research.google.com' --port=8888  --NotebookApp.port_retries=0\n",
        "2.   Open the docker\n",
        "3.   Connect to (Top right list) http://localhost:8888/....\n",
        "\n"
      ],
      "metadata": {
        "id": "msazqiiDB_dL"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2qgfVCswOljq",
        "outputId": "4ebf726f-7e0a-4ad9-d9fc-20f2a49fdab8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: psycopg2 in c:\\users\\22600\\anaconda\\lib\\site-packages (2.9.6)\n",
            "Requirement already satisfied: requests in c:\\users\\22600\\anaconda\\lib\\site-packages (2.28.1)\n",
            "Requirement already satisfied: charset-normalizer<3,>=2 in c:\\users\\22600\\anaconda\\lib\\site-packages (from requests) (2.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\22600\\anaconda\\lib\\site-packages (from requests) (2022.9.14)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\22600\\anaconda\\lib\\site-packages (from requests) (1.26.11)\n",
            "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\22600\\anaconda\\lib\\site-packages (from requests) (3.3)\n"
          ]
        }
      ],
      "source": [
        "!pip install psycopg2 requests"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import json"
      ],
      "metadata": {
        "id": "3t7pgx5kjn_k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "headers = {\n",
        "    \"User-Agent\": \"victor149286573@gmail.com\",\n",
        "    \"Accept-Encoding\": \"gzip, deflate\",\n",
        "    \"Host\": \"www.sec.gov\"\n",
        "}"
      ],
      "metadata": {
        "id": "7lZPdMbDi7C-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import psycopg2\n",
        "\n",
        "# Connect to your local PostgreSQL database\n",
        "def connect_to_database():\n",
        "    try:\n",
        "        connection = psycopg2.connect(\n",
        "          host=\"localhost\",\n",
        "          database=\"postgres\",\n",
        "          user=\"postgres\",\n",
        "          password=\"example\",\n",
        "        )\n",
        "        print(\"Connected to the database.\")\n",
        "        return connection\n",
        "    except (Exception, psycopg2.Error) as error:\n",
        "        print(\"Error while connecting to PostgreSQL:\", error)\n",
        "        return None\n",
        "\n",
        "# Call the function to connect to the database\n",
        "connection = connect_to_database()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DNLXpKg3Onad",
        "outputId": "3385ee36-7775-401a-ea81-482b3b54a16d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Connected to the database.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "\n",
        "#How to get the website page url?\n",
        "#From website page url, get the infotable\n",
        "#get from function: get_links_for_cik\n",
        "def extract_infotable_xml_url(url):\n",
        "    try:\n",
        "        # Send a GET request to the URL\n",
        "        response = requests.get(url,headers=headers) #headers=headers\n",
        "\n",
        "        # Check if the request was successful (status code 200)\n",
        "        if response.status_code == 200:\n",
        "            # Parse the HTML content with BeautifulSoup\n",
        "            soup = BeautifulSoup(response.text, 'html.parser')\n",
        "\n",
        "            # Find the anchor tag with href containing 'infotable.xml'\n",
        "            infotable_link = soup.find('a', href=lambda href: href and (\"xslForm13F_X02/infotable.xml\" in href or \"xslForm13F_X01/infotable.xml\" in href or \"xslForm13F_X01/form13fInfoTable.xml\" in href))\n",
        "            print(infotable_link)\n",
        "            if infotable_link:\n",
        "                infotable_url = \"https://www.sec.gov\"+infotable_link['href']\n",
        "                print(f\"URL for infotable.xml: {infotable_url}\")\n",
        "                return infotable_url\n",
        "            else:\n",
        "                print(\"infotable.xml not found in the HTML source.\")\n",
        "                return None\n",
        "        else:\n",
        "            print(f\"Failed to retrieve the page. Status code: {response.status_code}\")\n",
        "            return None\n",
        "    except requests.exceptions.RequestException as e:\n",
        "        print(f\"An error occurred: {e}\")\n",
        "        return None\n",
        "\n",
        "url=\"https://www.sec.gov/Archives/edgar/data/0001350694000114036117031111/0001140361-17-031111-index.html\"\n",
        "extract_infotable_xml_url(url)\n",
        "\n",
        "url_xml = extract_infotable_xml_url(url)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yI0gkU53k7-a",
        "outputId": "6468e5dc-b256-43cb-b10a-0c1e01733557"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<a href=\"/Archives/edgar/data/1350694/000114036117031111/xslForm13F_X01/form13fInfoTable.xml\">form13fInfoTable.html</a>\n",
            "URL for infotable.xml: https://www.sec.gov/Archives/edgar/data/1350694/000114036117031111/xslForm13F_X01/form13fInfoTable.xml\n",
            "<a href=\"/Archives/edgar/data/1350694/000114036117031111/xslForm13F_X01/form13fInfoTable.xml\">form13fInfoTable.html</a>\n",
            "URL for infotable.xml: https://www.sec.gov/Archives/edgar/data/1350694/000114036117031111/xslForm13F_X01/form13fInfoTable.xml\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "import xml.etree.ElementTree as ET\n",
        "\n",
        "headers = {\n",
        "    \"User-Agent\": \"victor149286573@gmail.com\",\n",
        "    \"Accept-Encoding\": \"gzip, deflate\",\n",
        "    \"Host\": \"www.sec.gov\"\n",
        "}\n",
        "\n",
        "def get_links_for_cik(cik):\n",
        "    base_url = \"https://www.sec.gov/Archives/edgar/data/\"\n",
        "    url = f\"{base_url}{cik}/index.xml\"\n",
        "\n",
        "    response = requests.get(url, headers=headers)\n",
        "    xml_content = response.content.decode(\"UTF-8\")\n",
        "\n",
        "    # Parse the XML data\n",
        "    root = ET.fromstring(xml_content)\n",
        "\n",
        "    # Find all the \"href\" elements\n",
        "    href_elements = root.findall(\".//href\")\n",
        "\n",
        "    # Extract the links\n",
        "    links = [elem.text for elem in href_elements]\n",
        "\n",
        "    result_links = []\n",
        "\n",
        "    # Process each link\n",
        "    for link in links:\n",
        "        # Split the URL by the last slash\n",
        "        parts = link.rsplit(\"/\", 1)\n",
        "        last_part = parts[-1]\n",
        "\n",
        "        # Parse into three parts\n",
        "        part1 = last_part[:10]\n",
        "        part2 = last_part[10:12]\n",
        "        part3 = last_part[12:]\n",
        "\n",
        "        if int(part2) < 15:\n",
        "            print(\"Breaking the loop as Year is less than 15.\")\n",
        "            break\n",
        "\n",
        "        new_link = f\"https://www.sec.gov/Archives/edgar/data/{cik}/{last_part}/{part1}-{part2}-{part3}-index.html\"\n",
        "        result_links.append(new_link)\n",
        "\n",
        "    return result_links\n",
        "\n",
        "# Call the function with your CIK number\n",
        "cik = \"0001350694\"  # Replace with your CIK number\n",
        "links = get_links_for_cik(cik)\n",
        "\n",
        "# Print the resulting links\n",
        "for link in links:\n",
        "    print(link)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3RoHTfxXbWbm",
        "outputId": "95d32fb0-6b02-4726-f882-b48e9583ba12"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Breaking the loop as Year is less than 15.\n",
            "https://www.sec.gov/Archives/edgar/data/0001350694/000117266123002051/0001172661-23-002051-index.html\n",
            "https://www.sec.gov/Archives/edgar/data/0001350694/000117266123000737/0001172661-23-000737-index.html\n",
            "https://www.sec.gov/Archives/edgar/data/0001350694/000117266122002357/0001172661-22-002357-index.html\n",
            "https://www.sec.gov/Archives/edgar/data/0001350694/000117266122001788/0001172661-22-001788-index.html\n",
            "https://www.sec.gov/Archives/edgar/data/0001350694/000117266122001289/0001172661-22-001289-index.html\n",
            "https://www.sec.gov/Archives/edgar/data/0001350694/000117266122000562/0001172661-22-000562-index.html\n",
            "https://www.sec.gov/Archives/edgar/data/0001350694/000117266121002162/0001172661-21-002162-index.html\n",
            "https://www.sec.gov/Archives/edgar/data/0001350694/000117266121001668/0001172661-21-001668-index.html\n",
            "https://www.sec.gov/Archives/edgar/data/0001350694/000117266121001153/0001172661-21-001153-index.html\n",
            "https://www.sec.gov/Archives/edgar/data/0001350694/000156761921003290/0001567619-21-003290-index.html\n",
            "https://www.sec.gov/Archives/edgar/data/0001350694/000156761920019382/0001567619-20-019382-index.html\n",
            "https://www.sec.gov/Archives/edgar/data/0001350694/000156761920014895/0001567619-20-014895-index.html\n",
            "https://www.sec.gov/Archives/edgar/data/0001350694/000156761920010047/0001567619-20-010047-index.html\n",
            "https://www.sec.gov/Archives/edgar/data/0001350694/000156761920003264/0001567619-20-003264-index.html\n",
            "https://www.sec.gov/Archives/edgar/data/0001350694/000156761919021161/0001567619-19-021161-index.html\n",
            "https://www.sec.gov/Archives/edgar/data/0001350694/000156761919016421/0001567619-19-016421-index.html\n",
            "https://www.sec.gov/Archives/edgar/data/0001350694/000156761919010822/0001567619-19-010822-index.html\n",
            "https://www.sec.gov/Archives/edgar/data/0001350694/000156761919003499/0001567619-19-003499-index.html\n",
            "https://www.sec.gov/Archives/edgar/data/0001350694/000156761918005788/0001567619-18-005788-index.html\n",
            "https://www.sec.gov/Archives/edgar/data/0001350694/000156761918000648/0001567619-18-000648-index.html\n",
            "https://www.sec.gov/Archives/edgar/data/0001350694/000156761918000646/0001567619-18-000646-index.html\n",
            "https://www.sec.gov/Archives/edgar/data/0001350694/000156761918000074/0001567619-18-000074-index.html\n",
            "https://www.sec.gov/Archives/edgar/data/0001350694/000114036118023865/0001140361-18-023865-index.html\n",
            "https://www.sec.gov/Archives/edgar/data/0001350694/000114036118007141/0001140361-18-007141-index.html\n",
            "https://www.sec.gov/Archives/edgar/data/0001350694/000114036117041998/0001140361-17-041998-index.html\n",
            "https://www.sec.gov/Archives/edgar/data/0001350694/000114036117031111/0001140361-17-031111-index.html\n",
            "https://www.sec.gov/Archives/edgar/data/0001350694/000114036117019802/0001140361-17-019802-index.html\n",
            "https://www.sec.gov/Archives/edgar/data/0001350694/000114036117005803/0001140361-17-005803-index.html\n",
            "https://www.sec.gov/Archives/edgar/data/0001350694/000114036116085573/0001140361-16-085573-index.html\n",
            "https://www.sec.gov/Archives/edgar/data/0001350694/000114036116075565/0001140361-16-075565-index.html\n",
            "https://www.sec.gov/Archives/edgar/data/0001350694/000114036116065095/0001140361-16-065095-index.html\n",
            "https://www.sec.gov/Archives/edgar/data/0001350694/000114036116051801/0001140361-16-051801-index.html\n",
            "https://www.sec.gov/Archives/edgar/data/0001350694/000114036115040647/0001140361-15-040647-index.html\n",
            "https://www.sec.gov/Archives/edgar/data/0001350694/000114036115030974/0001140361-15-030974-index.html\n",
            "https://www.sec.gov/Archives/edgar/data/0001350694/000114036115019529/0001140361-15-019529-index.html\n",
            "https://www.sec.gov/Archives/edgar/data/0001350694/000114036115005578/0001140361-15-005578-index.html\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "headers"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-bS7dB5W9DlR",
        "outputId": "6fb09074-91df-4871-e8f7-d7344d3158ce"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'User-Agent': 'victor149286573@gmail.com',\n",
              " 'Accept-Encoding': 'gzip, deflate',\n",
              " 'Host': 'www.sec.gov'}"
            ]
          },
          "metadata": {},
          "execution_count": 106
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import pandas as pd\n",
        "\n",
        "def extract_table_data(url):\n",
        "    try:\n",
        "        # Send an HTTP GET request and get the content of the page\n",
        "        response = requests.get(url,headers=headers)\n",
        "        response.raise_for_status()  # Raise an exception for 4xx/5xx status codes\n",
        "        html_content = response.text\n",
        "\n",
        "        # Parse the HTML using BeautifulSoup\n",
        "        soup = BeautifulSoup(html_content, 'html.parser')\n",
        "\n",
        "        # Find the table in the HTML using the 'summary' attribute\n",
        "        table = soup.find('table', summary='Form 13F-NT Header Information')\n",
        "\n",
        "        if not table:\n",
        "            print(\"Table not found on the page.\")\n",
        "            return None\n",
        "\n",
        "        # Initialize a list to store the extracted data from the table\n",
        "        data = []\n",
        "\n",
        "        # Loop through the rows of the table (skipping the header row)\n",
        "        for row in table.find_all('tr')[1:]:\n",
        "            # Extract the data from each cell in the row\n",
        "            cells = row.find_all('td')\n",
        "            row_data = [cell.text.strip() for cell in cells]\n",
        "\n",
        "            # Handle the case when the number of cells is 13 or 12\n",
        "\n",
        "            if len(cells) == 13:\n",
        "                selected_data = [row_data[0], row_data[2], row_data[4], row_data[5]]\n",
        "            elif len(cells) == 12:\n",
        "                selected_data = [row_data[0], row_data[2], row_data[3], row_data[4]]\n",
        "            else:\n",
        "                print(\"Unexpected number of cells in the row.\")\n",
        "                continue\n",
        "\n",
        "            data.append(selected_data)\n",
        "\n",
        "        # Create a DataFrame using the data and name the columns\n",
        "        columns = [\"NAME OF ISSUER\", \"CUSIP\", \"VALUE\", \"SHARE\"]\n",
        "        df = pd.DataFrame(data, columns=columns)\n",
        "\n",
        "        return df\n",
        "\n",
        "    except requests.exceptions.RequestException as e:\n",
        "        print(f\"Failed to retrieve the page. Error: {e}\")\n",
        "        return None\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # URL to fetch the HTML source code\n",
        "    url = 'https://www.sec.gov/Archives/edgar/data/1350694/000117266123002051/xslForm13F_X02/infotable.xml'\n",
        "\n",
        "    # Extract data from the table\n",
        "    table_df = extract_table_data(url)\n",
        "\n",
        "    # Print the extracted data\n",
        "    if table_df is not None:\n",
        "        print(table_df)\n",
        "    else:\n",
        "        print(\"Failed to extract data from the table.\")\n"
      ],
      "metadata": {
        "id": "C68iY9Cd24ra",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "82bf3158-d816-46e2-a5ac-5d0476dd271c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Unexpected number of cells in the row.\n",
            "                   NAME OF ISSUER      CUSIP                    VALUE  \\\n",
            "0                  NAME OF ISSUER      CUSIP  (to the nearest dollar)   \n",
            "1                360 DIGITECH INC  88557W101               23,092,363   \n",
            "2                           3M CO  88579Y101                8,775,949   \n",
            "3                     ABBOTT LABS  002824100              218,590,165   \n",
            "4                      ABBVIE INC  00287Y109               65,569,599   \n",
            "..                            ...        ...                      ...   \n",
            "706              ZILLOW GROUP INC  98954M200               26,214,887   \n",
            "707    ZIMMER BIOMET HOLDINGS INC  98956P102               37,929,244   \n",
            "708                    ZOETIS INC  98978V103                6,804,067   \n",
            "709  ZOOM VIDEO COMMUNICATIONS IN  98980L101                4,233,321   \n",
            "710        ZTO EXPRESS CAYMAN INC  98980A105               22,118,584   \n",
            "\n",
            "         SHARE  \n",
            "0      PRN AMT  \n",
            "1    1,190,328  \n",
            "2       83,493  \n",
            "3    2,158,702  \n",
            "4      411,430  \n",
            "..         ...  \n",
            "706    589,496  \n",
            "707    293,570  \n",
            "708     40,880  \n",
            "709     57,331  \n",
            "710    771,758  \n",
            "\n",
            "[711 rows x 4 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def create_filings_table(connection):\n",
        "    try:\n",
        "        cursor = connection.cursor()\n",
        "\n",
        "        create_table_query = \"\"\"\n",
        "        CREATE TABLE IF NOT EXISTS filings (\n",
        "            id SERIAL PRIMARY KEY,\n",
        "            company_name TEXT,\n",
        "            filing_date DATE,\n",
        "            filing_data JSONB\n",
        "        );\n",
        "        \"\"\"\n",
        "        cursor.execute(create_table_query)\n",
        "        connection.commit()\n",
        "        print(\"Table created successfully\")\n",
        "\n",
        "    except (Exception, psycopg2.Error) as error:\n",
        "        print(\"Error while creating the table:\", error)\n",
        "\n",
        "# Call the function to create the table\n",
        "create_filings_table(connection)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vJXI2CQrOybc",
        "outputId": "b70306eb-b395-4c10-ef1e-c1f637fa6a3d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Table created successfully\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Main\n",
        "\n",
        "import requests\n",
        "import xml.etree.ElementTree as ET\n",
        "import pandas as pd\n",
        "\n",
        "# Example: Set your CIK number\n",
        "cik = \"0001350694\"\n",
        "\n",
        "# Step 1: Get a list of URLs for infotable.xml files\n",
        "links = get_links_for_cik(cik)\n",
        "\n",
        "# Step 2-4: Fetch data and create DataFrames for each infotable.xml file\n",
        "data_frames = []\n",
        "\n",
        "for url in links:\n",
        "    infotable = extract_infotable_xml_url(url)\n",
        "    print(infotable)\n",
        "\n",
        "    response = requests.get(infotable,headers=headers)\n",
        "    # Parse the HTML content with BeautifulSoup\n",
        "    soup = BeautifulSoup(response.text, 'html.parser')\n",
        "\n",
        "    # Find all the table rows (tr) in the HTML\n",
        "    table_rows = soup.find_all(\"tr\")\n",
        "\n",
        "    # Extract the data from the table rows and create a list of dictionaries\n",
        "    data = []\n",
        "    for row in table_rows:\n",
        "       # Find all the table cells (td) in each row\n",
        "        cells = row.find_all(\"td\")\n",
        "        if len(cells) == 13:  # Ensure we have 13 cells per row (the correct table)\n",
        "           row_data = [cell.text.strip() for cell in cells]\n",
        "           data.append(row_data)\n",
        "\n",
        "# Create a DataFrame from the list of dictionaries\n",
        "    df = pd.DataFrame(data, columns=[\"Name of Issuer\", \"Title of Class\", \"CUSIP\", \"FIGI\", \"VALUE\", \"SHRS OR PRN AMT\",\n",
        "                                     \"PRN\", \"CALL\", \"DISCRETION\", \"MANAGER\", \"SOLE\", \"SHARED\", \"NONE\"])\n",
        "    print(df)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "cE-jwWQic4Z7",
        "outputId": "7470ec5f-503c-4f0e-d2f9-e15efacd0078"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Breaking the loop as Year is less than 15.\n",
            "<a href=\"/Archives/edgar/data/1350694/000117266123002051/xslForm13F_X02/infotable.xml\">infotable.html</a>\n",
            "URL for infotable.xml: https://www.sec.gov/Archives/edgar/data/1350694/000117266123002051/xslForm13F_X02/infotable.xml\n",
            "https://www.sec.gov/Archives/edgar/data/1350694/000117266123002051/xslForm13F_X02/infotable.xml\n",
            "                   Name of Issuer   Title of Class      CUSIP  FIGI  \\\n",
            "0                  NAME OF ISSUER   TITLE OF CLASS      CUSIP  FIGI   \n",
            "1                360 DIGITECH INC     AMERICAN DEP  88557W101     Â   \n",
            "2                           3M CO              COM  88579Y101     Â   \n",
            "3                     ABBOTT LABS              COM  002824100     Â   \n",
            "4                      ABBVIE INC              COM  00287Y109     Â   \n",
            "..                            ...              ...        ...   ...   \n",
            "706              ZILLOW GROUP INC     CL C CAP STK  98954M200     Â   \n",
            "707    ZIMMER BIOMET HOLDINGS INC              COM  98956P102     Â   \n",
            "708                    ZOETIS INC             CL A  98978V103     Â   \n",
            "709  ZOOM VIDEO COMMUNICATIONS IN             CL A  98980L101     Â   \n",
            "710        ZTO EXPRESS CAYMAN INC  SPONSORED ADS A  98980A105     Â   \n",
            "\n",
            "                       VALUE SHRS OR PRN AMT  PRN  CALL  DISCRETION  MANAGER  \\\n",
            "0    (to the nearest dollar)         PRN AMT  PRN  CALL  DISCRETION  MANAGER   \n",
            "1                 23,092,363       1,190,328   SH     Â        SOLE        Â   \n",
            "2                  8,775,949          83,493   SH     Â        SOLE        Â   \n",
            "3                218,590,165       2,158,702   SH     Â        SOLE        Â   \n",
            "4                 65,569,599         411,430   SH     Â        SOLE        Â   \n",
            "..                       ...             ...  ...   ...         ...      ...   \n",
            "706               26,214,887         589,496   SH     Â        SOLE        Â   \n",
            "707               37,929,244         293,570   SH     Â        SOLE        Â   \n",
            "708                6,804,067          40,880   SH     Â        SOLE        Â   \n",
            "709                4,233,321          57,331   SH     Â        SOLE        Â   \n",
            "710               22,118,584         771,758   SH     Â        SOLE        Â   \n",
            "\n",
            "          SOLE  SHARED   NONE  \n",
            "0         SOLE  SHARED   NONE  \n",
            "1    1,190,328       0      0  \n",
            "2       83,493       0      0  \n",
            "3    2,149,764       0  8,938  \n",
            "4      411,430       0      0  \n",
            "..         ...     ...    ...  \n",
            "706    589,496       0      0  \n",
            "707    292,483       0  1,087  \n",
            "708     40,880       0      0  \n",
            "709     57,331       0      0  \n",
            "710    771,758       0      0  \n",
            "\n",
            "[711 rows x 13 columns]\n",
            "<a href=\"/Archives/edgar/data/1350694/000117266123000737/xslForm13F_X02/infotable.xml\">infotable.html</a>\n",
            "URL for infotable.xml: https://www.sec.gov/Archives/edgar/data/1350694/000117266123000737/xslForm13F_X02/infotable.xml\n",
            "https://www.sec.gov/Archives/edgar/data/1350694/000117266123000737/xslForm13F_X02/infotable.xml\n",
            "                   Name of Issuer   Title of Class      CUSIP  FIGI  \\\n",
            "0                  NAME OF ISSUER   TITLE OF CLASS      CUSIP  FIGI   \n",
            "1                10X GENOMICS INC         CL A COM  88025U109     Â   \n",
            "2            1LIFE HEALTHCARE INC              COM  68269G107     Â   \n",
            "3                360 DIGITECH INC     AMERICAN DEP  88557W101     Â   \n",
            "4                           3M CO              COM  88579Y101     Â   \n",
            "..                            ...              ...        ...   ...   \n",
            "816      ZIONS BANCORPORATION N A              COM  989701107     Â   \n",
            "817                    ZOETIS INC             CL A  98978V103     Â   \n",
            "818  ZOOM VIDEO COMMUNICATIONS IN             CL A  98980L101     Â   \n",
            "819     ZOOMINFO TECHNOLOGIES INC     COMMON STOCK  98980F104     Â   \n",
            "820        ZTO EXPRESS CAYMAN INC  SPONSORED ADS A  98980A105     Â   \n",
            "\n",
            "                       VALUE SHRS OR PRN AMT  PRN  CALL  DISCRETION  MANAGER  \\\n",
            "0    (to the nearest dollar)         PRN AMT  PRN  CALL  DISCRETION  MANAGER   \n",
            "1                    475,323          13,044   SH     Â        SOLE        Â   \n",
            "2                    331,008          19,809   SH     Â        SOLE        Â   \n",
            "3                 25,306,218       1,242,938   SH     Â        SOLE        Â   \n",
            "4                  3,448,420          28,756   SH     Â        SOLE        Â   \n",
            "..                       ...             ...  ...   ...         ...      ...   \n",
            "816                2,659,605          54,101   SH     Â        SOLE        Â   \n",
            "817                4,055,039          27,670   SH     Â        SOLE        Â   \n",
            "818                  483,799           7,142   SH     Â        SOLE        Â   \n",
            "819                2,805,680          93,181   SH     Â        SOLE        Â   \n",
            "820               36,849,464       1,371,398   SH     Â        SOLE        Â   \n",
            "\n",
            "          SOLE  SHARED  NONE  \n",
            "0         SOLE  SHARED  NONE  \n",
            "1       13,044       0     0  \n",
            "2       19,508       0   301  \n",
            "3    1,242,938       0     0  \n",
            "4       28,756       0     0  \n",
            "..         ...     ...   ...  \n",
            "816     54,101       0     0  \n",
            "817     27,670       0     0  \n",
            "818      7,142       0     0  \n",
            "819     93,181       0     0  \n",
            "820  1,371,398       0     0  \n",
            "\n",
            "[821 rows x 13 columns]\n",
            "<a href=\"/Archives/edgar/data/1350694/000117266122002357/xslForm13F_X01/infotable.xml\">infotable.html</a>\n",
            "URL for infotable.xml: https://www.sec.gov/Archives/edgar/data/1350694/000117266122002357/xslForm13F_X01/infotable.xml\n",
            "https://www.sec.gov/Archives/edgar/data/1350694/000117266122002357/xslForm13F_X01/infotable.xml\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_2292\\411203272.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     18\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minfotable\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 20\u001b[1;33m     \u001b[0mresponse\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrequests\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minfotable\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mheaders\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mheaders\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     21\u001b[0m     \u001b[1;31m# Parse the HTML content with BeautifulSoup\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m     \u001b[0msoup\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mBeautifulSoup\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'html.parser'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\anaconda\\lib\\site-packages\\requests\\api.py\u001b[0m in \u001b[0;36mget\u001b[1;34m(url, params, **kwargs)\u001b[0m\n\u001b[0;32m     71\u001b[0m     \"\"\"\n\u001b[0;32m     72\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 73\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mrequest\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"get\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0murl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mparams\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     74\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     75\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\anaconda\\lib\\site-packages\\requests\\api.py\u001b[0m in \u001b[0;36mrequest\u001b[1;34m(method, url, **kwargs)\u001b[0m\n\u001b[0;32m     57\u001b[0m     \u001b[1;31m# cases, and look like a memory leak in others.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     58\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0msessions\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSession\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0msession\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 59\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0msession\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmethod\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmethod\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0murl\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0murl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     60\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     61\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\anaconda\\lib\\site-packages\\requests\\sessions.py\u001b[0m in \u001b[0;36mrequest\u001b[1;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[0;32m    585\u001b[0m         }\n\u001b[0;32m    586\u001b[0m         \u001b[0msend_kwargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msettings\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 587\u001b[1;33m         \u001b[0mresp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprep\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0msend_kwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    588\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    589\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mresp\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\anaconda\\lib\\site-packages\\requests\\sessions.py\u001b[0m in \u001b[0;36msend\u001b[1;34m(self, request, **kwargs)\u001b[0m\n\u001b[0;32m    699\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    700\u001b[0m         \u001b[1;31m# Send the request\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 701\u001b[1;33m         \u001b[0mr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0madapter\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrequest\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    702\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    703\u001b[0m         \u001b[1;31m# Total elapsed time of the request (approximately)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\anaconda\\lib\\site-packages\\requests\\adapters.py\u001b[0m in \u001b[0;36msend\u001b[1;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[0;32m    454\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    455\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 456\u001b[1;33m             \u001b[0mconn\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_connection\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrequest\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0murl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mproxies\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    457\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mLocationValueError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    458\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mInvalidURL\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrequest\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mrequest\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\anaconda\\lib\\site-packages\\requests\\adapters.py\u001b[0m in \u001b[0;36mget_connection\u001b[1;34m(self, url, proxies)\u001b[0m\n\u001b[0;32m    350\u001b[0m                     \u001b[1;34m\"and could be missing the host.\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    351\u001b[0m                 )\n\u001b[1;32m--> 352\u001b[1;33m             \u001b[0mproxy_manager\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mproxy_manager_for\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mproxy\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    353\u001b[0m             \u001b[0mconn\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mproxy_manager\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconnection_from_url\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0murl\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    354\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\anaconda\\lib\\site-packages\\requests\\adapters.py\u001b[0m in \u001b[0;36mproxy_manager_for\u001b[1;34m(self, proxy, **proxy_kwargs)\u001b[0m\n\u001b[0;32m    226\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    227\u001b[0m             \u001b[0mproxy_headers\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mproxy_headers\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mproxy\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 228\u001b[1;33m             manager = self.proxy_manager[proxy] = proxy_from_url(\n\u001b[0m\u001b[0;32m    229\u001b[0m                 \u001b[0mproxy\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    230\u001b[0m                 \u001b[0mproxy_headers\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mproxy_headers\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\anaconda\\lib\\site-packages\\urllib3\\poolmanager.py\u001b[0m in \u001b[0;36mproxy_from_url\u001b[1;34m(url, **kw)\u001b[0m\n\u001b[0;32m    535\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    536\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mproxy_from_url\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0murl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 537\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mProxyManager\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mproxy_url\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0murl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[1;32m~\\anaconda\\lib\\site-packages\\urllib3\\poolmanager.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, proxy_url, num_pools, headers, proxy_headers, proxy_ssl_context, use_forwarding_for_https, **connection_pool_kw)\u001b[0m\n\u001b[0;32m    494\u001b[0m         \u001b[0mconnection_pool_kw\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"_proxy_config\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mproxy_config\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    495\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 496\u001b[1;33m         \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mProxyManager\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnum_pools\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mheaders\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mconnection_pool_kw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    497\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    498\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mconnection_from_host\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhost\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mport\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscheme\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"http\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpool_kwargs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\anaconda\\lib\\site-packages\\urllib3\\poolmanager.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, num_pools, headers, **connection_pool_kw)\u001b[0m\n\u001b[0;32m    172\u001b[0m         \u001b[0mRequestMethods\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mheaders\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    173\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconnection_pool_kw\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mconnection_pool_kw\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 174\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpools\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mRecentlyUsedContainer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnum_pools\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdispose_func\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mp\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    175\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    176\u001b[0m         \u001b[1;31m# Locally set the pool classes and keys so other PoolManagers can\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\anaconda\\lib\\site-packages\\urllib3\\_collections.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, maxsize, dispose_func)\u001b[0m\n\u001b[0;32m     49\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdispose_func\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdispose_func\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     50\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 51\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_container\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mContainerCls\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     52\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlock\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mRLock\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     53\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Main\n",
        "\n",
        "import requests\n",
        "import xml.etree.ElementTree as ET\n",
        "import pandas as pd"
      ],
      "metadata": {
        "id": "ipPkafnU2ESs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_quarterly_dates():\n",
        "    start_year = 2023\n",
        "    end_year = pd.Timestamp.now().year\n",
        "    quarters = [(year, quarter) for year in range(start_year, end_year + 1) for quarter in range(1, 5)]\n",
        "    quarter_dates = [f\"Q{quarter}-{year}\" for year, quarter in quarters if year < end_year or quarter <= (pd.Timestamp.now().quarter)]\n",
        "    return quarter_dates"
      ],
      "metadata": {
        "id": "XMrc8PAuEdCT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to write DataFrame to an Excel file\n",
        "def write_dataframe_to_excel(df, excel_file):\n",
        "    with pd.ExcelWriter(excel_file) as writer:\n",
        "        df.to_excel(writer, index=False)\n",
        "\n",
        "#cik = 0001350694\n",
        "# Sample usage\n",
        "if __name__ == \"__main__\":\n",
        "    # Get CIK from user input\n",
        "    cik = input(\"Enter the CIK: \")\n",
        "    excel_file_path = 'D:/3 Projects/wmin project/output.xlsx'\n",
        "\n",
        "    links = get_links_for_cik(cik)\n",
        "    for url in links:\n",
        "        infotable = extract_infotable_xml_url(url)\n",
        "\n",
        "        df = extract_table_data(infotable)\n",
        "        print(df)\n",
        "        if df is not None:\n",
        "            # Write DataFrame into the Excel file\n",
        "            write_dataframe_to_excel(df, excel_file_path)\n",
        "\n",
        "            print(\"Data has been written to the Excel file.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 137
        },
        "id": "oslh6nxZ5ReR",
        "outputId": "df7c2c8c-3ee3-48bf-f487-12e33c5d30b2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "IndentationError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[1;36m  File \u001b[1;32m\"C:\\Users\\22600\\AppData\\Local\\Temp\\ipykernel_2292\\224705243.py\"\u001b[1;36m, line \u001b[1;32m20\u001b[0m\n\u001b[1;33m    if __name__ == \"__main__\":\u001b[0m\n\u001b[1;37m    ^\u001b[0m\n\u001b[1;31mIndentationError\u001b[0m\u001b[1;31m:\u001b[0m expected an indented block\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Function to generate quarterly dates from Q1-2023 to Q1-2015\n",
        "def generate_quarterly_dates():\n",
        "    quarters = [(year, quarter) for year in range(2023, 2014, -1) for quarter in range(1, 5)]\n",
        "    quarter_dates = [f\"Q{quarter}-{year}\" for year, quarter in quarters]\n",
        "    return quarter_dates\n",
        "\n",
        "# Function to write DataFrame to an Excel file\n",
        "def write_dataframe_to_excel(df, sheet_name, excel_file):\n",
        "    with pd.ExcelWriter(excel_file, engine='openpyxl', mode='a') as writer:\n",
        "        df.to_excel(writer, sheet_name=sheet_name, index=False)\n",
        "\n",
        "# Sample usage\n",
        "if __name__ == \"__main__\":\n",
        "    # Get CIK from user input\n",
        "    cik = \"0001350694\"\n",
        "#   cik = input(\"Enter the CIK: \")\n",
        "    excel_file_path = 'D:/3 Projects/wmin project/output.xlsx'\n",
        "\n",
        "    links = get_links_for_cik(cik)\n",
        "    quarterly_dates = generate_quarterly_dates()\n",
        "\n",
        "    # Create a new Excel file\n",
        "    pd.DataFrame().to_excel(excel_file_path, index=False)\n",
        "\n",
        "    for url, quarter_date in zip(links, quarterly_dates):\n",
        "        infotable = extract_infotable_xml_url(url)\n",
        "        df = extract_table_data(infotable)\n",
        "        print(1)\n",
        "        if df is not None:\n",
        "            # Write DataFrame into the Excel file\n",
        "            write_dataframe_to_excel(df, quarter_date, excel_file_path)\n",
        "\n",
        "            print(\"Data has been written to the Excel file.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2ZguDOqIKipT",
        "outputId": "6447c3fc-ebaa-4917-d5e6-6f83800bde40"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Breaking the loop as Year is less than 15.\n",
            "<a href=\"/Archives/edgar/data/1350694/000117266123002051/xslForm13F_X02/infotable.xml\">infotable.html</a>\n",
            "URL for infotable.xml: https://www.sec.gov/Archives/edgar/data/1350694/000117266123002051/xslForm13F_X02/infotable.xml\n",
            "Unexpected number of cells in the row.\n",
            "1\n",
            "Data has been written to the Excel file.\n",
            "<a href=\"/Archives/edgar/data/1350694/000117266123000737/xslForm13F_X02/infotable.xml\">infotable.html</a>\n",
            "URL for infotable.xml: https://www.sec.gov/Archives/edgar/data/1350694/000117266123000737/xslForm13F_X02/infotable.xml\n",
            "Unexpected number of cells in the row.\n",
            "1\n",
            "Data has been written to the Excel file.\n",
            "<a href=\"/Archives/edgar/data/1350694/000117266122002357/xslForm13F_X01/infotable.xml\">infotable.html</a>\n",
            "URL for infotable.xml: https://www.sec.gov/Archives/edgar/data/1350694/000117266122002357/xslForm13F_X01/infotable.xml\n",
            "Unexpected number of cells in the row.\n",
            "1\n",
            "Data has been written to the Excel file.\n",
            "<a href=\"/Archives/edgar/data/1350694/000117266122001788/xslForm13F_X01/infotable.xml\">infotable.html</a>\n",
            "URL for infotable.xml: https://www.sec.gov/Archives/edgar/data/1350694/000117266122001788/xslForm13F_X01/infotable.xml\n",
            "Unexpected number of cells in the row.\n",
            "1\n",
            "Data has been written to the Excel file.\n",
            "<a href=\"/Archives/edgar/data/1350694/000117266122001289/xslForm13F_X01/infotable.xml\">infotable.html</a>\n",
            "URL for infotable.xml: https://www.sec.gov/Archives/edgar/data/1350694/000117266122001289/xslForm13F_X01/infotable.xml\n",
            "Unexpected number of cells in the row.\n",
            "1\n",
            "Data has been written to the Excel file.\n",
            "<a href=\"/Archives/edgar/data/1350694/000117266122000562/xslForm13F_X01/infotable.xml\">infotable.html</a>\n",
            "URL for infotable.xml: https://www.sec.gov/Archives/edgar/data/1350694/000117266122000562/xslForm13F_X01/infotable.xml\n",
            "Unexpected number of cells in the row.\n",
            "1\n",
            "Data has been written to the Excel file.\n",
            "<a href=\"/Archives/edgar/data/1350694/000117266121002162/xslForm13F_X01/infotable.xml\">infotable.html</a>\n",
            "URL for infotable.xml: https://www.sec.gov/Archives/edgar/data/1350694/000117266121002162/xslForm13F_X01/infotable.xml\n",
            "Unexpected number of cells in the row.\n",
            "1\n",
            "Data has been written to the Excel file.\n",
            "<a href=\"/Archives/edgar/data/1350694/000117266121001668/xslForm13F_X01/infotable.xml\">infotable.html</a>\n",
            "URL for infotable.xml: https://www.sec.gov/Archives/edgar/data/1350694/000117266121001668/xslForm13F_X01/infotable.xml\n",
            "Unexpected number of cells in the row.\n",
            "1\n",
            "Data has been written to the Excel file.\n",
            "<a href=\"/Archives/edgar/data/1350694/000117266121001153/xslForm13F_X01/infotable.xml\">infotable.html</a>\n",
            "URL for infotable.xml: https://www.sec.gov/Archives/edgar/data/1350694/000117266121001153/xslForm13F_X01/infotable.xml\n",
            "Unexpected number of cells in the row.\n",
            "1\n",
            "Data has been written to the Excel file.\n",
            "<a href=\"/Archives/edgar/data/1350694/000156761921003290/xslForm13F_X01/form13fInfoTable.xml\">form13fInfoTable.html</a>\n",
            "URL for infotable.xml: https://www.sec.gov/Archives/edgar/data/1350694/000156761921003290/xslForm13F_X01/form13fInfoTable.xml\n",
            "Unexpected number of cells in the row.\n",
            "1\n",
            "Data has been written to the Excel file.\n",
            "<a href=\"/Archives/edgar/data/1350694/000156761920019382/xslForm13F_X01/form13fInfoTable.xml\">form13fInfoTable.html</a>\n",
            "URL for infotable.xml: https://www.sec.gov/Archives/edgar/data/1350694/000156761920019382/xslForm13F_X01/form13fInfoTable.xml\n",
            "Unexpected number of cells in the row.\n",
            "1\n",
            "Data has been written to the Excel file.\n",
            "<a href=\"/Archives/edgar/data/1350694/000156761920014895/xslForm13F_X01/form13fInfoTable.xml\">form13fInfoTable.html</a>\n",
            "URL for infotable.xml: https://www.sec.gov/Archives/edgar/data/1350694/000156761920014895/xslForm13F_X01/form13fInfoTable.xml\n",
            "Unexpected number of cells in the row.\n",
            "1\n",
            "Data has been written to the Excel file.\n",
            "<a href=\"/Archives/edgar/data/1350694/000156761920010047/xslForm13F_X01/form13fInfoTable.xml\">form13fInfoTable.html</a>\n",
            "URL for infotable.xml: https://www.sec.gov/Archives/edgar/data/1350694/000156761920010047/xslForm13F_X01/form13fInfoTable.xml\n",
            "Unexpected number of cells in the row.\n",
            "1\n",
            "Data has been written to the Excel file.\n",
            "<a href=\"/Archives/edgar/data/1350694/000156761920003264/xslForm13F_X01/form13fInfoTable.xml\">form13fInfoTable.html</a>\n",
            "URL for infotable.xml: https://www.sec.gov/Archives/edgar/data/1350694/000156761920003264/xslForm13F_X01/form13fInfoTable.xml\n",
            "Unexpected number of cells in the row.\n",
            "1\n",
            "Data has been written to the Excel file.\n",
            "<a href=\"/Archives/edgar/data/1350694/000156761919021161/xslForm13F_X01/form13fInfoTable.xml\">form13fInfoTable.html</a>\n",
            "URL for infotable.xml: https://www.sec.gov/Archives/edgar/data/1350694/000156761919021161/xslForm13F_X01/form13fInfoTable.xml\n",
            "Unexpected number of cells in the row.\n",
            "1\n",
            "Data has been written to the Excel file.\n",
            "<a href=\"/Archives/edgar/data/1350694/000156761919016421/xslForm13F_X01/form13fInfoTable.xml\">form13fInfoTable.html</a>\n",
            "URL for infotable.xml: https://www.sec.gov/Archives/edgar/data/1350694/000156761919016421/xslForm13F_X01/form13fInfoTable.xml\n",
            "Unexpected number of cells in the row.\n",
            "1\n",
            "Data has been written to the Excel file.\n",
            "<a href=\"/Archives/edgar/data/1350694/000156761919010822/xslForm13F_X01/form13fInfoTable.xml\">form13fInfoTable.html</a>\n",
            "URL for infotable.xml: https://www.sec.gov/Archives/edgar/data/1350694/000156761919010822/xslForm13F_X01/form13fInfoTable.xml\n",
            "Unexpected number of cells in the row.\n",
            "1\n",
            "Data has been written to the Excel file.\n",
            "<a href=\"/Archives/edgar/data/1350694/000156761919003499/xslForm13F_X01/form13fInfoTable.xml\">form13fInfoTable.html</a>\n",
            "URL for infotable.xml: https://www.sec.gov/Archives/edgar/data/1350694/000156761919003499/xslForm13F_X01/form13fInfoTable.xml\n",
            "Unexpected number of cells in the row.\n",
            "1\n",
            "Data has been written to the Excel file.\n",
            "<a href=\"/Archives/edgar/data/1350694/000156761918005788/xslForm13F_X01/form13fInfoTable.xml\">form13fInfoTable.html</a>\n",
            "URL for infotable.xml: https://www.sec.gov/Archives/edgar/data/1350694/000156761918005788/xslForm13F_X01/form13fInfoTable.xml\n",
            "Unexpected number of cells in the row.\n",
            "1\n",
            "Data has been written to the Excel file.\n",
            "<a href=\"/Archives/edgar/data/1350694/000156761918000648/xslForm13F_X01/form13fInfoTable.xml\">form13fInfoTable.html</a>\n",
            "URL for infotable.xml: https://www.sec.gov/Archives/edgar/data/1350694/000156761918000648/xslForm13F_X01/form13fInfoTable.xml\n",
            "Unexpected number of cells in the row.\n",
            "1\n",
            "Data has been written to the Excel file.\n",
            "<a href=\"/Archives/edgar/data/1350694/000156761918000646/xslForm13F_X01/form13fInfoTable.xml\">form13fInfoTable.html</a>\n",
            "URL for infotable.xml: https://www.sec.gov/Archives/edgar/data/1350694/000156761918000646/xslForm13F_X01/form13fInfoTable.xml\n",
            "Unexpected number of cells in the row.\n",
            "1\n",
            "Data has been written to the Excel file.\n",
            "<a href=\"/Archives/edgar/data/1350694/000156761918000074/xslForm13F_X01/form13fInfoTable.xml\">form13fInfoTable.html</a>\n",
            "URL for infotable.xml: https://www.sec.gov/Archives/edgar/data/1350694/000156761918000074/xslForm13F_X01/form13fInfoTable.xml\n",
            "Unexpected number of cells in the row.\n",
            "1\n",
            "Data has been written to the Excel file.\n",
            "<a href=\"/Archives/edgar/data/1350694/000114036118023865/xslForm13F_X01/form13fInfoTable.xml\">form13fInfoTable.html</a>\n",
            "URL for infotable.xml: https://www.sec.gov/Archives/edgar/data/1350694/000114036118023865/xslForm13F_X01/form13fInfoTable.xml\n",
            "Unexpected number of cells in the row.\n",
            "1\n",
            "Data has been written to the Excel file.\n",
            "<a href=\"/Archives/edgar/data/1350694/000114036118007141/xslForm13F_X01/form13fInfoTable.xml\">form13fInfoTable.html</a>\n",
            "URL for infotable.xml: https://www.sec.gov/Archives/edgar/data/1350694/000114036118007141/xslForm13F_X01/form13fInfoTable.xml\n",
            "Unexpected number of cells in the row.\n",
            "1\n",
            "Data has been written to the Excel file.\n",
            "<a href=\"/Archives/edgar/data/1350694/000114036117041998/xslForm13F_X01/form13fInfoTable.xml\">form13fInfoTable.html</a>\n",
            "URL for infotable.xml: https://www.sec.gov/Archives/edgar/data/1350694/000114036117041998/xslForm13F_X01/form13fInfoTable.xml\n",
            "Unexpected number of cells in the row.\n",
            "1\n",
            "Data has been written to the Excel file.\n",
            "<a href=\"/Archives/edgar/data/1350694/000114036117031111/xslForm13F_X01/form13fInfoTable.xml\">form13fInfoTable.html</a>\n",
            "URL for infotable.xml: https://www.sec.gov/Archives/edgar/data/1350694/000114036117031111/xslForm13F_X01/form13fInfoTable.xml\n",
            "Unexpected number of cells in the row.\n",
            "1\n",
            "Data has been written to the Excel file.\n",
            "<a href=\"/Archives/edgar/data/1350694/000114036117019802/xslForm13F_X01/form13fInfoTable.xml\">form13fInfoTable.html</a>\n",
            "URL for infotable.xml: https://www.sec.gov/Archives/edgar/data/1350694/000114036117019802/xslForm13F_X01/form13fInfoTable.xml\n",
            "Unexpected number of cells in the row.\n",
            "1\n",
            "Data has been written to the Excel file.\n",
            "<a href=\"/Archives/edgar/data/1350694/000114036117005803/xslForm13F_X01/form13fInfoTable.xml\">form13fInfoTable.html</a>\n",
            "URL for infotable.xml: https://www.sec.gov/Archives/edgar/data/1350694/000114036117005803/xslForm13F_X01/form13fInfoTable.xml\n",
            "Unexpected number of cells in the row.\n",
            "1\n",
            "Data has been written to the Excel file.\n",
            "<a href=\"/Archives/edgar/data/1350694/000114036116085573/xslForm13F_X01/form13fInfoTable.xml\">form13fInfoTable.html</a>\n",
            "URL for infotable.xml: https://www.sec.gov/Archives/edgar/data/1350694/000114036116085573/xslForm13F_X01/form13fInfoTable.xml\n",
            "Unexpected number of cells in the row.\n",
            "1\n",
            "Data has been written to the Excel file.\n",
            "<a href=\"/Archives/edgar/data/1350694/000114036116075565/xslForm13F_X01/form13fInfoTable.xml\">form13fInfoTable.html</a>\n",
            "URL for infotable.xml: https://www.sec.gov/Archives/edgar/data/1350694/000114036116075565/xslForm13F_X01/form13fInfoTable.xml\n",
            "Unexpected number of cells in the row.\n",
            "1\n",
            "Data has been written to the Excel file.\n",
            "<a href=\"/Archives/edgar/data/1350694/000114036116065095/xslForm13F_X01/form13fInfoTable.xml\">form13fInfoTable.html</a>\n",
            "URL for infotable.xml: https://www.sec.gov/Archives/edgar/data/1350694/000114036116065095/xslForm13F_X01/form13fInfoTable.xml\n",
            "Unexpected number of cells in the row.\n",
            "1\n",
            "Data has been written to the Excel file.\n",
            "<a href=\"/Archives/edgar/data/1350694/000114036116051801/xslForm13F_X01/form13fInfoTable.xml\">form13fInfoTable.html</a>\n",
            "URL for infotable.xml: https://www.sec.gov/Archives/edgar/data/1350694/000114036116051801/xslForm13F_X01/form13fInfoTable.xml\n",
            "Unexpected number of cells in the row.\n",
            "1\n",
            "Data has been written to the Excel file.\n",
            "<a href=\"/Archives/edgar/data/1350694/000114036115040647/xslForm13F_X01/form13fInfoTable.xml\">form13fInfoTable.html</a>\n",
            "URL for infotable.xml: https://www.sec.gov/Archives/edgar/data/1350694/000114036115040647/xslForm13F_X01/form13fInfoTable.xml\n",
            "Unexpected number of cells in the row.\n",
            "1\n",
            "Data has been written to the Excel file.\n",
            "<a href=\"/Archives/edgar/data/1350694/000114036115030974/xslForm13F_X01/form13fInfoTable.xml\">form13fInfoTable.html</a>\n",
            "URL for infotable.xml: https://www.sec.gov/Archives/edgar/data/1350694/000114036115030974/xslForm13F_X01/form13fInfoTable.xml\n",
            "Unexpected number of cells in the row.\n",
            "1\n",
            "Data has been written to the Excel file.\n",
            "<a href=\"/Archives/edgar/data/1350694/000114036115019529/xslForm13F_X01/form13fInfoTable.xml\">form13fInfoTable.html</a>\n",
            "URL for infotable.xml: https://www.sec.gov/Archives/edgar/data/1350694/000114036115019529/xslForm13F_X01/form13fInfoTable.xml\n",
            "Unexpected number of cells in the row.\n",
            "1\n",
            "Data has been written to the Excel file.\n",
            "<a href=\"/Archives/edgar/data/1350694/000114036115005578/xslForm13F_X01/form13fInfoTable.xml\">form13fInfoTable.html</a>\n",
            "URL for infotable.xml: https://www.sec.gov/Archives/edgar/data/1350694/000114036115005578/xslForm13F_X01/form13fInfoTable.xml\n",
            "Unexpected number of cells in the row.\n",
            "1\n",
            "Data has been written to the Excel file.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import pandas as pd\n",
        "import re\n",
        "\n",
        "# Function to extract data from the HTML table and return a DataFrame\n",
        "def extract_table_data(url, quarter):\n",
        "    try:\n",
        "        response = requests.get(url,headers=headers)\n",
        "        response.raise_for_status()\n",
        "        html_content = response.text\n",
        "        soup = BeautifulSoup(html_content, 'html.parser')\n",
        "\n",
        "        # Find the table in the HTML\n",
        "        table = soup.find('table', summary='Form 13F-NT Header Information')\n",
        "        if not table:\n",
        "            print(\"Table not found on the page.\")\n",
        "            return None\n",
        "\n",
        "        # Initialize a list to store the extracted data from the table\n",
        "        data = []\n",
        "\n",
        "        # Loop through the rows of the table\n",
        "        for row in table.find_all('tr'):\n",
        "            cells = row.find_all('td')\n",
        "            # Check if it's a header row (number of cells == 11 or 12)\n",
        "            if len(cells) in [11, 12]:\n",
        "                columns = [cell.text.strip() for cell in cells]\n",
        "                if len(columns) == 11:\n",
        "                    # If 11 columns, add an empty value for the missing column\n",
        "                    columns.insert(2, '')\n",
        "                data.append(columns)\n",
        "            elif len(cells) == 13:\n",
        "                # If 13 columns, select the appropriate columns (0, 2, 4, 5)\n",
        "                columns = [cells[0].text.strip(), cells[2].text.strip(), cells[4].text.strip(), cells[5].text.strip()]\n",
        "                data.append(columns)\n",
        "\n",
        "        df = pd.DataFrame(data[1:], columns=data[0])  # Skip the header row\n",
        "\n",
        "        # Add the 'Quarter' column with the extracted quarter information\n",
        "        df['Quarter'] = quarter\n",
        "\n",
        "        # If the number of columns is less than 12, fill in empty values for the missing columns\n",
        "        if len(df.columns) < 12:\n",
        "            missing_columns = 12 - len(df.columns)\n",
        "            for _ in range(missing_columns):\n",
        "                df[f'Missing_{_+1}'] = ''\n",
        "\n",
        "        return df\n",
        "\n",
        "    except requests.exceptions.RequestException as e:\n",
        "        print(f\"Failed to retrieve the page. Error: {e}\")\n",
        "        return None\n",
        "\n",
        "# Rest of the code remains the same...\n",
        "\n",
        "\n",
        "# Function to generate quarterly dates from Q1-2023 to Q1-2015\n",
        "def generate_quarterly_dates():\n",
        "    quarters = []\n",
        "    current_year = 2023\n",
        "    current_quarter = 1\n",
        "\n",
        "    while current_year >= 2015:\n",
        "        quarter_str = f'Q{current_quarter}-{current_year}'\n",
        "        quarters.append(quarter_str)\n",
        "\n",
        "        current_quarter += 1\n",
        "        if current_quarter > 4:\n",
        "            current_quarter = 1\n",
        "            current_year -= 1\n",
        "\n",
        "    return quarters\n",
        "\n",
        "# Rest of the code remains the same...\n",
        "\n",
        "# Sample usage\n",
        "if __name__ == \"__main__\":\n",
        "    # Get CIK from user input\n",
        "    cik = input(\"Enter the CIK: \")\n",
        "    excel_file_path = 'D:/3 Projects/wmin project/output.xlsx'\n",
        "\n",
        "    links = get_links_for_cik(cik)\n",
        "    quarters = generate_quarterly_dates()\n",
        "    i = 0\n",
        "\n",
        "    # Initialize a DataFrame to store the combined data\n",
        "    combined_df = pd.DataFrame()\n",
        "\n",
        "    for url in links:\n",
        "        infotable = extract_infotable_xml_url(url)\n",
        "\n",
        "        df = extract_table_data(infotable, quarters[i])\n",
        "\n",
        "        if df is not None:\n",
        "            combined_df = combined_df.append(df, ignore_index=True)\n",
        "            i += 1\n",
        "            print(f\"Data from {quarters[i]} has been extracted.\")\n",
        "\n",
        "    # Write the combined DataFrame into the Excel file\n",
        "    write_dataframe_to_excel(combined_df, excel_file_path)\n",
        "\n",
        "    print(\"Data has been written to the Excel file.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "MmnSHmLxOAF8",
        "outputId": "b8e56dab-cb4f-4a6e-aacf-bb9cc6b10271"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter the CIK: 0001350694\n",
            "Breaking the loop as Year is less than 15.\n",
            "<a href=\"/Archives/edgar/data/1350694/000117266123002051/xslForm13F_X02/infotable.xml\">infotable.html</a>\n",
            "URL for infotable.xml: https://www.sec.gov/Archives/edgar/data/1350694/000117266123002051/xslForm13F_X02/infotable.xml\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_2292\\286317441.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     91\u001b[0m         \u001b[0minfotable\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mextract_infotable_xml_url\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0murl\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     92\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 93\u001b[1;33m         \u001b[0mdf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mextract_table_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minfotable\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mquarters\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     94\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     95\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mdf\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_2292\\286317441.py\u001b[0m in \u001b[0;36mextract_table_data\u001b[1;34m(url, quarter)\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mextract_table_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0murl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mquarter\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m         \u001b[0mresponse\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrequests\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0murl\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mheaders\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mheaders\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m         \u001b[0mresponse\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mraise_for_status\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m         \u001b[0mhtml_content\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mresponse\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\anaconda\\lib\\site-packages\\requests\\api.py\u001b[0m in \u001b[0;36mget\u001b[1;34m(url, params, **kwargs)\u001b[0m\n\u001b[0;32m     71\u001b[0m     \"\"\"\n\u001b[0;32m     72\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 73\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mrequest\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"get\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0murl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mparams\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     74\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     75\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\anaconda\\lib\\site-packages\\requests\\api.py\u001b[0m in \u001b[0;36mrequest\u001b[1;34m(method, url, **kwargs)\u001b[0m\n\u001b[0;32m     57\u001b[0m     \u001b[1;31m# cases, and look like a memory leak in others.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     58\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0msessions\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSession\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0msession\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 59\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0msession\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmethod\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmethod\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0murl\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0murl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     60\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     61\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\anaconda\\lib\\site-packages\\requests\\sessions.py\u001b[0m in \u001b[0;36mrequest\u001b[1;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[0;32m    585\u001b[0m         }\n\u001b[0;32m    586\u001b[0m         \u001b[0msend_kwargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msettings\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 587\u001b[1;33m         \u001b[0mresp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprep\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0msend_kwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    588\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    589\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mresp\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\anaconda\\lib\\site-packages\\requests\\sessions.py\u001b[0m in \u001b[0;36msend\u001b[1;34m(self, request, **kwargs)\u001b[0m\n\u001b[0;32m    699\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    700\u001b[0m         \u001b[1;31m# Send the request\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 701\u001b[1;33m         \u001b[0mr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0madapter\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrequest\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    702\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    703\u001b[0m         \u001b[1;31m# Total elapsed time of the request (approximately)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\anaconda\\lib\\site-packages\\requests\\adapters.py\u001b[0m in \u001b[0;36msend\u001b[1;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[0;32m    487\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    488\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mchunked\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 489\u001b[1;33m                 resp = conn.urlopen(\n\u001b[0m\u001b[0;32m    490\u001b[0m                     \u001b[0mmethod\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mrequest\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmethod\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    491\u001b[0m                     \u001b[0murl\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0murl\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\anaconda\\lib\\site-packages\\urllib3\\connectionpool.py\u001b[0m in \u001b[0;36murlopen\u001b[1;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw)\u001b[0m\n\u001b[0;32m    698\u001b[0m             )\n\u001b[0;32m    699\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mis_new_proxy_conn\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mhttp_tunnel_required\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 700\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_prepare_proxy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    701\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    702\u001b[0m             \u001b[1;31m# Make the request on the httplib connection object.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\anaconda\\lib\\site-packages\\urllib3\\connectionpool.py\u001b[0m in \u001b[0;36m_prepare_proxy\u001b[1;34m(self, conn)\u001b[0m\n\u001b[0;32m    994\u001b[0m             \u001b[0mconn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtls_in_tls_required\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    995\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 996\u001b[1;33m         \u001b[0mconn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconnect\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    997\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    998\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_new_conn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\anaconda\\lib\\site-packages\\urllib3\\connection.py\u001b[0m in \u001b[0;36mconnect\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    412\u001b[0m             \u001b[0mcontext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload_default_certs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    413\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 414\u001b[1;33m         self.sock = ssl_wrap_socket(\n\u001b[0m\u001b[0;32m    415\u001b[0m             \u001b[0msock\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mconn\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    416\u001b[0m             \u001b[0mkeyfile\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkey_file\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\anaconda\\lib\\site-packages\\urllib3\\util\\ssl_.py\u001b[0m in \u001b[0;36mssl_wrap_socket\u001b[1;34m(sock, keyfile, certfile, cert_reqs, ca_certs, server_hostname, ssl_version, ciphers, ssl_context, ca_cert_dir, key_password, ca_cert_data, tls_in_tls)\u001b[0m\n\u001b[0;32m    447\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    448\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0msend_sni\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 449\u001b[1;33m         ssl_sock = _ssl_wrap_socket_impl(\n\u001b[0m\u001b[0;32m    450\u001b[0m             \u001b[0msock\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcontext\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtls_in_tls\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mserver_hostname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mserver_hostname\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    451\u001b[0m         )\n",
            "\u001b[1;32m~\\anaconda\\lib\\site-packages\\urllib3\\util\\ssl_.py\u001b[0m in \u001b[0;36m_ssl_wrap_socket_impl\u001b[1;34m(sock, ssl_context, tls_in_tls, server_hostname)\u001b[0m\n\u001b[0;32m    491\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    492\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mserver_hostname\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 493\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mssl_context\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwrap_socket\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msock\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mserver_hostname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mserver_hostname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    494\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    495\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mssl_context\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwrap_socket\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msock\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\anaconda\\lib\\ssl.py\u001b[0m in \u001b[0;36mwrap_socket\u001b[1;34m(self, sock, server_side, do_handshake_on_connect, suppress_ragged_eofs, server_hostname, session)\u001b[0m\n\u001b[0;32m    499\u001b[0m         \u001b[1;31m# SSLSocket class handles server_hostname encoding before it calls\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    500\u001b[0m         \u001b[1;31m# ctx._wrap_socket()\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 501\u001b[1;33m         return self.sslsocket_class._create(\n\u001b[0m\u001b[0;32m    502\u001b[0m             \u001b[0msock\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msock\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    503\u001b[0m             \u001b[0mserver_side\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mserver_side\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\anaconda\\lib\\ssl.py\u001b[0m in \u001b[0;36m_create\u001b[1;34m(cls, sock, server_side, do_handshake_on_connect, suppress_ragged_eofs, server_hostname, context, session)\u001b[0m\n\u001b[0;32m   1039\u001b[0m                         \u001b[1;31m# non-blocking\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1040\u001b[0m                         \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"do_handshake_on_connect should not be specified for non-blocking sockets\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1041\u001b[1;33m                     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdo_handshake\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1042\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mOSError\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1043\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\anaconda\\lib\\ssl.py\u001b[0m in \u001b[0;36mdo_handshake\u001b[1;34m(self, block)\u001b[0m\n\u001b[0;32m   1308\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0.0\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mblock\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1309\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msettimeout\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1310\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_sslobj\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdo_handshake\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1311\u001b[0m         \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1312\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msettimeout\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Function to read data from the Excel file\n",
        "def read_data_from_excel(excel_file_path):\n",
        "    # Read each sheet from the Excel file and store the data in a dictionary\n",
        "    data_by_quarter = {}\n",
        "    with pd.ExcelFile(excel_file_path) as xls:\n",
        "        for sheet_name in xls.sheet_names:\n",
        "            # Assume sheet names are in the format Q1-2023, Q4-2022, etc.\n",
        "            quarter = sheet_name\n",
        "            df = pd.read_excel(xls, sheet_name=sheet_name)\n",
        "            data_by_quarter[quarter] = df\n",
        "    return data_by_quarter"
      ],
      "metadata": {
        "id": "doyCQg77vUgW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to write the merged data to a new Excel file\n",
        "def write_merged_data_to_excel(merged_df, excel_file_path):\n",
        "    # Write the merged DataFrame into a new Excel file\n",
        "    with pd.ExcelWriter(excel_file_path) as writer:\n",
        "        merged_df.to_excel(writer, index=False)"
      ],
      "metadata": {
        "id": "IjEsH_g3vece"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to merge data from all quarters\n",
        "def merge_data(data_by_quarter):\n",
        "    # Create a new DataFrame to store the merged data\n",
        "    merged_df = pd.DataFrame()\n",
        "\n",
        "    # Find common columns among all DataFrames\n",
        "    common_columns = set.union(*[set(df.columns) for df in data_by_quarter.values()])\n",
        "\n",
        "    # Check if 'NAME OF ISSUER' exists in the common columns\n",
        "    if 'NAME OF ISSUER' not in common_columns:\n",
        "        raise ValueError(\"Column 'NAME OF ISSUER' not found in the common columns.\")\n",
        "\n",
        "    # Iterate through the data_by_quarter dictionary and merge the data\n",
        "    for quarter, df in data_by_quarter.items():\n",
        "      # Filter the DataFrame to include only common columns\n",
        "      common_columns_in_df = common_columns.intersection(df.columns)\n",
        "\n",
        "      df = df[['NAME OF ISSUER','CUSIP','VALUE']]\n",
        "\n",
        "      # Merge the data into the new DataFrame using the company name (NAME OF ISSUER) as the key\n",
        "      if merged_df.empty:\n",
        "         merged_df = df\n",
        "      else:\n",
        "         # Merge the data into the new DataFrame using the company name (NAME OF ISSUER) as the key\n",
        "         merged_df = pd.merge(merged_df, df[['CUSIP','VALUE']], how='outer', on='CUSIP')\n",
        "\n",
        "\n",
        "    return merged_df\n",
        "\n",
        "# Sample usage\n",
        "if __name__ == \"__main__\":\n",
        "    excel_file_path = 'D:/3 Projects/wmin project/bridgewater.xlsx'\n",
        "\n",
        "    # Step 1: Read data from the Excel file\n",
        "    data_by_quarter = read_data_from_excel(excel_file_path)\n",
        "    #print(data_by_quarter)\n",
        "    # Step 2: Merge data from all quarters\n",
        "    merged_df = merge_data(data_by_quarter)\n",
        "\n",
        "    # Step 3: Write the merged data to a new Excel file\n",
        "    new_excel_file_path = 'D:/3 Projects/wmin project/merged_output.xlsx'\n",
        "    write_merged_data_to_excel(merged_df, new_excel_file_path)\n",
        "\n",
        "    print(\"Data has been merged and written to the new Excel file.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MJ8qbbeNPZ_e",
        "outputId": "46e07c7a-f9ce-49fd-ddae-8f74813cec31"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "C:\\Users\\22600\\AppData\\Local\\Temp\\ipykernel_2292\\2809170885.py:25: FutureWarning: Passing 'suffixes' which cause duplicate columns {'VALUE_x'} in the result is deprecated and will raise a MergeError in a future version.\n",
            "  merged_df = pd.merge(merged_df, df[['CUSIP','VALUE']], how='outer', on='CUSIP')\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data has been merged and written to the new Excel file.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "    data_by_quarter = read_data_from_excel(\"D:/3 Projects/wmin project/bridgewater.xlsx\")\n",
        "    #print(data_by_quarter)\n",
        "\n",
        "# Create a new DataFrame to store the merged data\n",
        "    merged_df = pd.DataFrame()\n",
        "\n",
        "    # Find common columns among all DataFrames\n",
        "    common_columns = set.union(*[set(df.columns) for df in data_by_quarter.values()])\n",
        "    print(common_columns)\n",
        "    # Check if 'NAME OF ISSUER' exists in the common columns\n",
        "    if 'NAME OF ISSUER' not in common_columns:\n",
        "        raise ValueError(\"Column 'NAME OF ISSUER' not found in the common columns.\")\n",
        "\n",
        "    # Iterate through the data_by_quarter dictionary and merge the data\n",
        "    for quarter, df in data_by_quarter.items():\n",
        "      # Filter the DataFrame to include only common columns\n",
        "      common_columns_in_df = common_columns.intersection(df.columns)\n",
        "\n",
        "      df = df[common_columns_in_df]\n",
        "      # Merge the data into the new DataFrame using the company name (NAME OF ISSUER) as the key\n",
        "      if merged_df.empty:\n",
        "         merged_df = df\n",
        "      else:\n",
        "         # Merge the data into the new DataFrame using the company name (NAME OF ISSUER) as the key\n",
        "         merged_df = pd.merge(merged_df, df[\"VALUE\"], how='left', on='NAME OF ISSUER')\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "bHQY9XeTvsj6",
        "outputId": "3a87d60e-5c25-46f2-d24d-965357a38326"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'CUSIP', 'NAME OF ISSUER', 'VALUE', 'SHARE'}\n",
            "Empty DataFrame\n",
            "Columns: []\n",
            "Index: []\n",
            "         CUSIP                NAME OF ISSUER                    VALUE  \\\n",
            "0        CUSIP                NAME OF ISSUER  (to the nearest dollar)   \n",
            "1    88557W101              360 DIGITECH INC               23,092,363   \n",
            "2    88579Y101                         3M CO                8,775,949   \n",
            "3    002824100                   ABBOTT LABS              218,590,165   \n",
            "4    00287Y109                    ABBVIE INC               65,569,599   \n",
            "..         ...                           ...                      ...   \n",
            "706  98954M200              ZILLOW GROUP INC               26,214,887   \n",
            "707  98956P102    ZIMMER BIOMET HOLDINGS INC               37,929,244   \n",
            "708  98978V103                    ZOETIS INC                6,804,067   \n",
            "709  98980L101  ZOOM VIDEO COMMUNICATIONS IN                4,233,321   \n",
            "710  98980A105        ZTO EXPRESS CAYMAN INC               22,118,584   \n",
            "\n",
            "         SHARE  \n",
            "0      PRN AMT  \n",
            "1    1,190,328  \n",
            "2       83,493  \n",
            "3    2,158,702  \n",
            "4      411,430  \n",
            "..         ...  \n",
            "706    589,496  \n",
            "707    293,570  \n",
            "708     40,880  \n",
            "709     57,331  \n",
            "710    771,758  \n",
            "\n",
            "[711 rows x 4 columns]\n",
            "2\n",
            "         CUSIP                NAME OF ISSUER                    VALUE  \\\n",
            "0        CUSIP                NAME OF ISSUER  (to the nearest dollar)   \n",
            "1    88557W101              360 DIGITECH INC               23,092,363   \n",
            "2    88579Y101                         3M CO                8,775,949   \n",
            "3    002824100                   ABBOTT LABS              218,590,165   \n",
            "4    00287Y109                    ABBVIE INC               65,569,599   \n",
            "..         ...                           ...                      ...   \n",
            "706  98954M200              ZILLOW GROUP INC               26,214,887   \n",
            "707  98956P102    ZIMMER BIOMET HOLDINGS INC               37,929,244   \n",
            "708  98978V103                    ZOETIS INC                6,804,067   \n",
            "709  98980L101  ZOOM VIDEO COMMUNICATIONS IN                4,233,321   \n",
            "710  98980A105        ZTO EXPRESS CAYMAN INC               22,118,584   \n",
            "\n",
            "         SHARE  \n",
            "0      PRN AMT  \n",
            "1    1,190,328  \n",
            "2       83,493  \n",
            "3    2,158,702  \n",
            "4      411,430  \n",
            "..         ...  \n",
            "706    589,496  \n",
            "707    293,570  \n",
            "708     40,880  \n",
            "709     57,331  \n",
            "710    771,758  \n",
            "\n",
            "[711 rows x 4 columns]\n",
            "1\n",
            "         CUSIP                NAME OF ISSUER                    VALUE  \\\n",
            "0        CUSIP                NAME OF ISSUER  (to the nearest dollar)   \n",
            "1    88557W101              360 DIGITECH INC               23,092,363   \n",
            "2    88579Y101                         3M CO                8,775,949   \n",
            "3    002824100                   ABBOTT LABS              218,590,165   \n",
            "4    00287Y109                    ABBVIE INC               65,569,599   \n",
            "..         ...                           ...                      ...   \n",
            "706  98954M200              ZILLOW GROUP INC               26,214,887   \n",
            "707  98956P102    ZIMMER BIOMET HOLDINGS INC               37,929,244   \n",
            "708  98978V103                    ZOETIS INC                6,804,067   \n",
            "709  98980L101  ZOOM VIDEO COMMUNICATIONS IN                4,233,321   \n",
            "710  98980A105        ZTO EXPRESS CAYMAN INC               22,118,584   \n",
            "\n",
            "         SHARE  \n",
            "0      PRN AMT  \n",
            "1    1,190,328  \n",
            "2       83,493  \n",
            "3    2,158,702  \n",
            "4      411,430  \n",
            "..         ...  \n",
            "706    589,496  \n",
            "707    293,570  \n",
            "708     40,880  \n",
            "709     57,331  \n",
            "710    771,758  \n",
            "\n",
            "[711 rows x 4 columns]\n",
            "         CUSIP                NAME OF ISSUER                    VALUE  \\\n",
            "0        CUSIP                NAME OF ISSUER  (to the nearest dollar)   \n",
            "1    88025U109              10X GENOMICS INC                  475,323   \n",
            "2    68269G107          1LIFE HEALTHCARE INC                  331,008   \n",
            "3    88557W101              360 DIGITECH INC               25,306,218   \n",
            "4    88579Y101                         3M CO                3,448,420   \n",
            "..         ...                           ...                      ...   \n",
            "816  989701107      ZIONS BANCORPORATION N A                2,659,605   \n",
            "817  98978V103                    ZOETIS INC                4,055,039   \n",
            "818  98980L101  ZOOM VIDEO COMMUNICATIONS IN                  483,799   \n",
            "819  98980F104     ZOOMINFO TECHNOLOGIES INC                2,805,680   \n",
            "820  98980A105        ZTO EXPRESS CAYMAN INC               36,849,464   \n",
            "\n",
            "         SHARE  \n",
            "0      PRN AMT  \n",
            "1       13,044  \n",
            "2       19,809  \n",
            "3    1,242,938  \n",
            "4       28,756  \n",
            "..         ...  \n",
            "816     54,101  \n",
            "817     27,670  \n",
            "818      7,142  \n",
            "819     93,181  \n",
            "820  1,371,398  \n",
            "\n",
            "[821 rows x 4 columns]\n",
            "2\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_2292\\3068689644.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     28\u001b[0m   \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     29\u001b[0m      \u001b[1;31m# Merge the data into the new DataFrame using the company name (NAME OF ISSUER) as the key\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 30\u001b[1;33m      \u001b[0mmerged_df\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmerge\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmerged_df\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"VALUE\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhow\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'left'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mon\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'NAME OF ISSUER'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     31\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\anaconda\\lib\\site-packages\\pandas\\core\\reshape\\merge.py\u001b[0m in \u001b[0;36mmerge\u001b[1;34m(left, right, how, on, left_on, right_on, left_index, right_index, sort, suffixes, copy, indicator, validate)\u001b[0m\n\u001b[0;32m    104\u001b[0m     \u001b[0mvalidate\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mstr\u001b[0m \u001b[1;33m|\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    105\u001b[0m ) -> DataFrame:\n\u001b[1;32m--> 106\u001b[1;33m     op = _MergeOperation(\n\u001b[0m\u001b[0;32m    107\u001b[0m         \u001b[0mleft\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    108\u001b[0m         \u001b[0mright\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\anaconda\\lib\\site-packages\\pandas\\core\\reshape\\merge.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, left, right, how, on, left_on, right_on, axis, left_index, right_index, sort, suffixes, copy, indicator, validate)\u001b[0m\n\u001b[0;32m    697\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mright_join_keys\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    698\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin_names\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 699\u001b[1;33m         ) = self._get_merge_keys()\n\u001b[0m\u001b[0;32m    700\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    701\u001b[0m         \u001b[1;31m# validate the merge keys dtypes. We may need to coerce\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\anaconda\\lib\\site-packages\\pandas\\core\\reshape\\merge.py\u001b[0m in \u001b[0;36m_get_merge_keys\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1094\u001b[0m                     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mis_rkey\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrk\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1095\u001b[0m                         \u001b[1;32mif\u001b[0m \u001b[0mrk\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1096\u001b[1;33m                             \u001b[0mright_keys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mright\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_label_or_level_values\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrk\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1097\u001b[0m                         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1098\u001b[0m                             \u001b[1;31m# work-around for merge_asof(right_index=True)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\anaconda\\lib\\site-packages\\pandas\\core\\generic.py\u001b[0m in \u001b[0;36m_get_label_or_level_values\u001b[1;34m(self, key, axis)\u001b[0m\n\u001b[0;32m   1777\u001b[0m             \u001b[0mvalues\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maxes\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_level_values\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_values\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1778\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1779\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1780\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1781\u001b[0m         \u001b[1;31m# Check for duplicates\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;31mKeyError\u001b[0m: 'NAME OF ISSUER'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df1 = pd.read_excel(\"D:/3 Projects/wmin project/bridgewater.xlsx\",sheet_name=\"Q1-2023\")\n",
        "df2 = pd.read_excel(\"D:/3 Projects/wmin project/bridgewater.xlsx\",sheet_name=\"Q2-2023\")\n",
        "merged_df = pd.merge(df1, df2[['NAME OF ISSUER','VALUE']], how='left', on='NAME OF ISSUER')\n",
        "print(merged_df)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xKLgW9mWw_P0",
        "outputId": "557e659d-6c04-4fde-cecb-0b072340c8af"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   NAME OF ISSUER                    VALUE\n",
            "0                  NAME OF ISSUER  (to the nearest dollar)\n",
            "1                10X GENOMICS INC                  475,323\n",
            "2            1LIFE HEALTHCARE INC                  331,008\n",
            "3                360 DIGITECH INC               25,306,218\n",
            "4                           3M CO                3,448,420\n",
            "..                            ...                      ...\n",
            "816      ZIONS BANCORPORATION N A                2,659,605\n",
            "817                    ZOETIS INC                4,055,039\n",
            "818  ZOOM VIDEO COMMUNICATIONS IN                  483,799\n",
            "819     ZOOMINFO TECHNOLOGIES INC                2,805,680\n",
            "820        ZTO EXPRESS CAYMAN INC               36,849,464\n",
            "\n",
            "[821 rows x 2 columns]\n",
            "                   NAME OF ISSUER      CUSIP                  VALUE_x  \\\n",
            "0                  NAME OF ISSUER      CUSIP  (to the nearest dollar)   \n",
            "1                360 DIGITECH INC  88557W101               23,092,363   \n",
            "2                           3M CO  88579Y101                8,775,949   \n",
            "3                     ABBOTT LABS  002824100              218,590,165   \n",
            "4                      ABBVIE INC  00287Y109               65,569,599   \n",
            "..                            ...        ...                      ...   \n",
            "763              ZILLOW GROUP INC  98954M200               26,214,887   \n",
            "764    ZIMMER BIOMET HOLDINGS INC  98956P102               37,929,244   \n",
            "765                    ZOETIS INC  98978V103                6,804,067   \n",
            "766  ZOOM VIDEO COMMUNICATIONS IN  98980L101                4,233,321   \n",
            "767        ZTO EXPRESS CAYMAN INC  98980A105               22,118,584   \n",
            "\n",
            "         SHARE                  VALUE_y  \n",
            "0      PRN AMT  (to the nearest dollar)  \n",
            "1    1,190,328               25,306,218  \n",
            "2       83,493                3,448,420  \n",
            "3    2,158,702              243,820,644  \n",
            "4      411,430                4,864,784  \n",
            "..         ...                      ...  \n",
            "763    589,496               36,221,820  \n",
            "764    293,570               40,353,750  \n",
            "765     40,880                4,055,039  \n",
            "766     57,331                  483,799  \n",
            "767    771,758               36,849,464  \n",
            "\n",
            "[768 rows x 5 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "current_directory = os.getcwd()\n",
        "current_directory"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fc_aFGKBJbnx",
        "outputId": "e1992423-8dca-4495-e019-88b34d975c2b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'C:\\\\Users\\\\22600'"
            ]
          },
          "metadata": {},
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def write_dataframe_to_excel(dataframes, file_path):\n",
        "    if not dataframes:\n",
        "        print(\"No data to write. Please provide valid DataFrames.\")\n",
        "        return\n",
        "\n",
        "    # Create a Pandas Excel writer object\n",
        "    writer = pd.ExcelWriter(file_path, engine='xlsxwriter')\n",
        "\n",
        "    # Get the current date\n",
        "    current_date = datetime.now()\n",
        "\n",
        "    # Sort the DataFrames in descending order of quarters\n",
        "    dataframes.sort(key=lambda df: df['Date'].iloc[0], reverse=True)\n",
        "\n",
        "    # Loop through the DataFrames and write each to a separate sheet\n",
        "    for i, df in enumerate(dataframes):\n",
        "        if df is not None:\n",
        "            # Generate the quarterly label based on the date from the DataFrame\n",
        "            date_str = df['Date'].iloc[0].strftime(\"%b-%Y\")\n",
        "            quarter_label = f\"Q{df['Date'].iloc[0].quarter}-{date_str}\"\n",
        "\n",
        "            # Name the sheet with the quarterly label (e.g., Q1-2023, Q4-2022, etc.)\n",
        "            sheet_name = f\"Sheet_{quarter_label}\"\n",
        "\n",
        "            # Save the DataFrame to the Excel sheet\n",
        "            df.to_excel(writer, sheet_name=sheet_name, index=False)\n",
        "\n",
        "    # Save and close the Excel writer\n",
        "    writer.save()\n",
        "    writer.close()\n",
        "\n",
        "    # Get and print the current working directory\n",
        "    current_directory = os.getcwd()\n",
        "    print(\"Current working directory:\", current_directory)\n",
        "\n",
        "    print(f\"DataFrames have been saved to {file_path}.\")\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # ... (same as before)\n",
        "\n",
        "    # List of URLs for different tables (replace with your actual URLs)\n",
        "    urls = [\n",
        "        'https://www.sec.gov/Archives/edgar/data/1350694/000117266123002051/xslForm13F_X02/infotable.xml',\n",
        "        # Add more URLs here for different tables\n",
        "    ]\n",
        "\n",
        "    # List to store DataFrames\n",
        "    dataframes = []\n",
        "\n",
        "    # Loop through the URLs and extract data from each table\n",
        "    for i, url in enumerate(urls):\n",
        "        table_df = extract_table_data(url)\n",
        "        if table_df is not None:\n",
        "            dataframes.append(table_df)\n",
        "\n",
        "    # Specify the desired path for saving the Excel file\n",
        "    excel_file_path = 'C:/Users/YourUsername/Documents/output.xlsx'\n",
        "\n",
        "    # Write the list of DataFrames to the Excel file\n",
        "    write_dataframe_to_excel(dataframes, excel_file_path)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 768
        },
        "id": "6FZbg8r_GQ0-",
        "outputId": "e781ce4b-4b98-40ed-9ace-bfbd1bb1377d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Unexpected number of cells in the row.\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_2292\\126389119.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     59\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     60\u001b[0m     \u001b[1;31m# Write the list of DataFrames to the Excel file\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 61\u001b[1;33m     \u001b[0mwrite_dataframe_to_excel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataframes\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mexcel_file_path\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_2292\\126389119.py\u001b[0m in \u001b[0;36mwrite_dataframe_to_excel\u001b[1;34m(dataframes, file_path)\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[1;31m# Create a Pandas Excel writer object\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m     \u001b[0mwriter\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mExcelWriter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfile_path\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'xlsxwriter'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m     \u001b[1;31m# Get the current date\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\anaconda\\lib\\site-packages\\pandas\\io\\excel\\_xlsxwriter.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, path, engine, date_format, datetime_format, mode, storage_options, if_sheet_exists, engine_kwargs, **kwargs)\u001b[0m\n\u001b[0;32m    189\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Append mode is not supported with xlsxwriter!\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    190\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 191\u001b[1;33m         super().__init__(\n\u001b[0m\u001b[0;32m    192\u001b[0m             \u001b[0mpath\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    193\u001b[0m             \u001b[0mengine\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\anaconda\\lib\\site-packages\\pandas\\io\\excel\\_base.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, path, engine, date_format, datetime_format, mode, storage_options, if_sheet_exists, engine_kwargs, **kwargs)\u001b[0m\n\u001b[0;32m    923\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhandles\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mIOHandles\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcast\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mBuffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpath\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcompression\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m{\u001b[0m\u001b[1;34m\"copression\"\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    924\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mExcelWriter\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 925\u001b[1;33m             self.handles = get_handle(\n\u001b[0m\u001b[0;32m    926\u001b[0m                 \u001b[0mpath\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstorage_options\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mstorage_options\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mis_text\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    927\u001b[0m             )\n",
            "\u001b[1;32m~\\anaconda\\lib\\site-packages\\pandas\\io\\common.py\u001b[0m in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    709\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    710\u001b[0m             \u001b[1;31m# Binary mode\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 711\u001b[1;33m             \u001b[0mhandle\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mioargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    712\u001b[0m         \u001b[0mhandles\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    713\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'C:/Users/YourUsername/Documents/output.xlsx'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.getcwd()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cUz6fduVBgqi",
        "outputId": "dc624d99-1bb5-464d-947b-c5246f9e090a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'C:\\\\Users\\\\22600'"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "    infotable = \"https://www.sec.gov/Archives/edgar/data/1350694/000117266122002357/xslForm13F_X01/infotable.xml\"\n",
        "\n",
        "    response = requests.get(infotable,headers=headers)\n",
        "    # Parse the HTML content with BeautifulSoup\n",
        "    soup = BeautifulSoup(response.text, 'html.parser')\n",
        "\n",
        "    # Find all the table rows (tr) in the HTML\n",
        "    table_rows = soup.find_all(\"tr\")\n",
        "\n",
        "    # Extract the data from the table rows and create a list of dictionaries\n",
        "    data = []\n",
        "    for row in table_rows:\n",
        "       # Find all the table cells (td) in each row\n",
        "        cells = row.find_all(\"td\")\n",
        "        print(cells)\n",
        "        if len(cells) == 13:  # Ensure we have 13 cells per row (the correct table)\n",
        "           row_data = [cell.text.strip() for cell in cells]\n",
        "           data.append(row_data)\n",
        "    print(data)\n",
        "# Create a DataFrame from the list of dictionaries\n",
        "    df = pd.DataFrame(data, columns=[\"Name of Issuer\", \"Title of Class\", \"CUSIP\", \"FIGI\", \"VALUE\", \"SHRS OR PRN AMT\",\n",
        "                                     \"PRN\", \"CALL\", \"DISCRETION\", \"MANAGER\", \"SOLE\", \"SHARED\", \"NONE\"])\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "n3Q9qDHGzVaz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "def extract_info_table_data(url):\n",
        "    try:\n",
        "        # Send an HTTP GET request and get the content of the page\n",
        "        response = requests.get(url)\n",
        "        response.raise_for_status()  # Raise an exception for 4xx/5xx status codes\n",
        "        html_content = response.text\n",
        "\n",
        "        # Parse the HTML using BeautifulSoup\n",
        "        soup = BeautifulSoup(html_content, 'html.parser')\n",
        "\n",
        "        # Find the table in the HTML using the 'summary' attribute\n",
        "        table = soup.find('table', summary='Form 13F-NT Header Information')\n",
        "\n",
        "        if not table:\n",
        "            print(\"Table not found on the page.\")\n",
        "            return None\n",
        "\n",
        "        # Initialize a list to store the extracted data from the table\n",
        "        data = []\n",
        "\n",
        "        # Loop through the rows of the table (skipping the header row)\n",
        "        for row in table.find_all('tr')[1:]:\n",
        "            # Extract the data from each cell in the row\n",
        "            cells = row.find_all('td')\n",
        "            row_data = [cell.text.strip() for cell in cells]\n",
        "            data.append(row_data)\n",
        "\n",
        "        # Create a DataFrame from the extracted data\n",
        "        df = pd.DataFrame(data, columns=[\"NAME OF ISSUER\", \"TITLE OF CLASS\", \"CUSIP\", \"(x$1000)\", \"PRN AMT\", \"PRN\",\n",
        "                                         \"CALL\", \"DISCRETION\", \"MANAGER\", \"SOLE\", \"SHARED\", \"NONE\"])\n",
        "\n",
        "        return df\n",
        "\n",
        "    except requests.exceptions.RequestException as e:\n",
        "        print(f\"Failed to retrieve the page. Error: {e}\")\n",
        "        return None\n",
        "\n",
        "\n",
        "# Function to write DataFrames into an Excel file with different sheets\n",
        "def write_dataframes_to_excel(dataframes, excel_file):\n",
        "    with pd.ExcelWriter(excel_file) as writer:\n",
        "        for cik, df in dataframes.items():\n",
        "            df.to_excel(writer, sheet_name=cik, index=False)\n",
        "\n",
        "# Sample usage\n",
        "if __name__ == \"__main__\":\n",
        "    # URLs with InfoTable data for different CIKs\n",
        "    cik_links = {\n",
        "        \"1350694\": \"https://www.sec.gov/Archives/edgar/data/1350694/000117266123002051/0001172661-23-002051-index.html\",\n",
        "        # Add more CIKs and links here\n",
        "    }\n",
        "\n",
        "    dataframes = {}\n",
        "    for cik, link in cik_links.items():\n",
        "        df = extract_info_table_data(link)\n",
        "        if df is not None:\n",
        "            dataframes[cik] = df\n",
        "\n",
        "    # File path to save the Excel file\n",
        "    excel_file_path = \"info_table_data.xlsx\"\n",
        "\n",
        "    # Write DataFrames into the Excel file with different sheets\n",
        "    write_dataframes_to_excel(dataframes, excel_file_path)\n",
        "\n",
        "    print(\"Data has been written to the Excel file.\")\n"
      ],
      "metadata": {
        "id": "cCzVefUN3m-S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "jjNtLOX2N-mL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Assuming you have a list of DataFrames named 'list_of_dataframes'\n",
        "# Replace 'output_folder' with the desired folder path\n",
        "output_folder = './output_folder/'\n",
        "\n",
        "# Loop through the list of DataFrames and create separate Excel files for each DataFrame\n",
        "for index, df in enumerate(list_of_dataframes):\n",
        "    # Generate the file name for each DataFrame, for example: 'dataframe_0.xlsx', 'dataframe_1.xlsx', etc.\n",
        "    output_file = f'{output_folder}dataframe_{index}.xlsx'\n",
        "\n",
        "    # Create an ExcelWriter object to write to the Excel file\n",
        "    with pd.ExcelWriter(output_file) as writer:\n",
        "        # Write the DataFrame to the sheet named 'Sheet1'\n",
        "        df.to_excel(writer, sheet_name='Sheet1', index=False)  # If you want to exclude the index column\n"
      ],
      "metadata": {
        "id": "q16Syc3xzUG5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 提取index，提取index对应每一季度的website link，提取每个link中对应的table"
      ],
      "metadata": {
        "id": "SW-989roqMEE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#store the data locally into a csv\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "# Send a request to fetch the HTML content\n",
        "response = requests.get(url_xml,headers=headers)\n",
        "\n",
        " # Find all the table rows (tr) in the HTML\n",
        "table_rows = soup.find_all(\"tr\")\n",
        "\n",
        "    # Extract the data from the table rows and create a list of dictionaries\n",
        "data = []\n",
        "for row in table_rows:\n",
        "    # Find all the table cells (td) in each row\n",
        "    cells = row.find_all(\"td\")\n",
        "    if len(cells) == 13:  # Ensure we have 13 cells per row (the correct table)\n",
        "        row_data = [cell.text.strip() for cell in cells]\n",
        "        data.append(row_data)\n",
        "\n",
        "# Create a DataFrame from the list of dictionaries\n",
        "    df = pd.DataFrame(data, columns=[\"Name of Issuer\", \"Title of Class\", \"CUSIP\", \"FIGI\", \"VALUE\", \"SHRS OR PRN AMT\",\n",
        "                                     \"PRN\", \"CALL\", \"DISCRETION\", \"MANAGER\", \"SOLE\", \"SHARED\", \"NONE\"])\n",
        "\n",
        "# Print the DataFrame\n",
        "print(df)\n",
        "# Store the DataFrame in a CSV file locally\n",
        "output_location = \"D:\\\\3 Projects\\\\wmin project\\\\CSV outcomes\\\\bridgewater.csv\"\n",
        "df.to_csv(output_location, index=False)\n",
        "\n",
        "print(f\"DataFrame saved to '{output_location}'\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "xSfNGL4WFgrX",
        "outputId": "0c167aa0-5ba8-4968-f21f-59d4bffe8289"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   Name of Issuer   Title of Class      CUSIP  FIGI  \\\n",
            "0                  NAME OF ISSUER   TITLE OF CLASS      CUSIP  FIGI   \n",
            "1                360 DIGITECH INC     AMERICAN DEP  88557W101         \n",
            "2                           3M CO              COM  88579Y101         \n",
            "3                     ABBOTT LABS              COM  002824100         \n",
            "4                      ABBVIE INC              COM  00287Y109         \n",
            "..                            ...              ...        ...   ...   \n",
            "706              ZILLOW GROUP INC     CL C CAP STK  98954M200         \n",
            "707    ZIMMER BIOMET HOLDINGS INC              COM  98956P102         \n",
            "708                    ZOETIS INC             CL A  98978V103         \n",
            "709  ZOOM VIDEO COMMUNICATIONS IN             CL A  98980L101         \n",
            "710        ZTO EXPRESS CAYMAN INC  SPONSORED ADS A  98980A105         \n",
            "\n",
            "                       VALUE SHRS OR PRN AMT  PRN  CALL  DISCRETION  MANAGER  \\\n",
            "0    (to the nearest dollar)         PRN AMT  PRN  CALL  DISCRETION  MANAGER   \n",
            "1                 23,092,363       1,190,328   SH              SOLE            \n",
            "2                  8,775,949          83,493   SH              SOLE            \n",
            "3                218,590,165       2,158,702   SH              SOLE            \n",
            "4                 65,569,599         411,430   SH              SOLE            \n",
            "..                       ...             ...  ...   ...         ...      ...   \n",
            "706               26,214,887         589,496   SH              SOLE            \n",
            "707               37,929,244         293,570   SH              SOLE            \n",
            "708                6,804,067          40,880   SH              SOLE            \n",
            "709                4,233,321          57,331   SH              SOLE            \n",
            "710               22,118,584         771,758   SH              SOLE            \n",
            "\n",
            "          SOLE  SHARED   NONE  \n",
            "0         SOLE  SHARED   NONE  \n",
            "1    1,190,328       0      0  \n",
            "2       83,493       0      0  \n",
            "3    2,149,764       0  8,938  \n",
            "4      411,430       0      0  \n",
            "..         ...     ...    ...  \n",
            "706    589,496       0      0  \n",
            "707    292,483       0  1,087  \n",
            "708     40,880       0      0  \n",
            "709     57,331       0      0  \n",
            "710    771,758       0      0  \n",
            "\n",
            "[711 rows x 13 columns]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "PermissionError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mPermissionError\u001b[0m                           Traceback (most recent call last)",
            "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_5528\\2662783605.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     26\u001b[0m \u001b[1;31m# Store the DataFrame in a CSV file locally\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     27\u001b[0m \u001b[0moutput_location\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"D:\\\\3 Projects\\\\wmin project\\\\CSV outcomes\\\\bridgewater.csv\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 28\u001b[1;33m \u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutput_location\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     29\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     30\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf\"DataFrame saved to '{output_location}'\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\anaconda\\lib\\site-packages\\pandas\\core\\generic.py\u001b[0m in \u001b[0;36mto_csv\u001b[1;34m(self, path_or_buf, sep, na_rep, float_format, columns, header, index, index_label, mode, encoding, compression, quoting, quotechar, line_terminator, chunksize, date_format, doublequote, escapechar, decimal, errors, storage_options)\u001b[0m\n\u001b[0;32m   3464\u001b[0m         )\n\u001b[0;32m   3465\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3466\u001b[1;33m         return DataFrameRenderer(formatter).to_csv(\n\u001b[0m\u001b[0;32m   3467\u001b[0m             \u001b[0mpath_or_buf\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3468\u001b[0m             \u001b[0mline_terminator\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mline_terminator\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\anaconda\\lib\\site-packages\\pandas\\io\\formats\\format.py\u001b[0m in \u001b[0;36mto_csv\u001b[1;34m(self, path_or_buf, encoding, sep, columns, index_label, mode, compression, quoting, quotechar, line_terminator, chunksize, date_format, doublequote, escapechar, errors, storage_options)\u001b[0m\n\u001b[0;32m   1103\u001b[0m             \u001b[0mformatter\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfmt\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1104\u001b[0m         )\n\u001b[1;32m-> 1105\u001b[1;33m         \u001b[0mcsv_formatter\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1106\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1107\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcreated_buffer\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\anaconda\\lib\\site-packages\\pandas\\io\\formats\\csvs.py\u001b[0m in \u001b[0;36msave\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    235\u001b[0m         \"\"\"\n\u001b[0;32m    236\u001b[0m         \u001b[1;31m# apply compression and byte/text conversion\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 237\u001b[1;33m         with get_handle(\n\u001b[0m\u001b[0;32m    238\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    239\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\anaconda\\lib\\site-packages\\pandas\\io\\common.py\u001b[0m in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    700\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[1;32mand\u001b[0m \u001b[1;34m\"b\"\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    701\u001b[0m             \u001b[1;31m# Encoding\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 702\u001b[1;33m             handle = open(\n\u001b[0m\u001b[0;32m    703\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    704\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;31mPermissionError\u001b[0m: [Errno 13] Permission denied: 'D:\\\\3 Projects\\\\wmin project\\\\CSV outcomes\\\\bridgewater.csv'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import json\n",
        "\n",
        "xml_content = []\n",
        "headers = {\n",
        "    \"User-Agent\": \"victor149286573@gmail.com\",\n",
        "    \"Accept-Encoding\": \"gzip, deflate\",\n",
        "    \"Host\": \"www.sec.gov\"\n",
        "}\n",
        "# Function to download 13F filings and store in the database\n",
        "def download_and_store_filings(connection):\n",
        "    base_url = \"https://www.sec.gov/Archives/edgar/data/\"\n",
        "    cik = \"0001350694\"  # Replace with your CIK number\n",
        "    url = f\"{base_url}{cik}/index.xml\"\n",
        "\n",
        "    response = requests.get(url,headers=headers)\n",
        "    xml_content = response.content.decode(\"UTF-8\")\n",
        "    print(xml_content)\n",
        "   # print(response.content.decode(\"UTF-8\"))\n",
        "    if response.status_code == 200:\n",
        "\n",
        "# Call the function to download and store filings\n",
        "download_and_store_filings(connection)\n"
      ],
      "metadata": {
        "id": "Plq1J3jvO2UA",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 209
        },
        "outputId": "84be3873-6721-4d15-ab1b-50227e08709e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_5528\\4243607158.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     52\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     53\u001b[0m \u001b[1;31m# Call the function to download and store filings\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 54\u001b[1;33m \u001b[0mdownload_and_store_filings\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconnection\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mcik\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[1;31mTypeError\u001b[0m: download_and_store_filings() takes 1 positional argument but 2 were given"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "if connection:\n",
        "    connection.close()\n",
        "    print(\"Database connection closed.\")\n"
      ],
      "metadata": {
        "id": "MeQ2ijf_O_zI"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}